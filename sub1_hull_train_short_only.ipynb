{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 111543,
          "databundleVersionId": 14861981,
          "sourceType": "competition"
        },
        {
          "sourceId": 14148402,
          "sourceType": "datasetVersion",
          "datasetId": 9016539
        },
        {
          "sourceId": 286046598,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "sub1_hull_train_short_only",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fyenne/salaryshowoff/blob/master/sub1_hull_train_short_only.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "5Mtu_Q6ndrME"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "hull_tactical_market_prediction_path = kagglehub.competition_download('hull-tactical-market-prediction')\n",
        "fyenneyenn_hull_feature_select_dict_path = kagglehub.dataset_download('fyenneyenn/hull-feature-select-dict')\n",
        "fyenneyenn_download_talib_path = kagglehub.notebook_output_download('fyenneyenn/download-talib')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "9-ZCOiYudrMG"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install --system --no-index -U --no-deps --find-links='/kaggle/input/download-talib'  ta_lib"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-14T05:27:03.451413Z",
          "iopub.execute_input": "2025-12-14T05:27:03.451862Z",
          "iopub.status.idle": "2025-12-14T05:27:04.177485Z",
          "shell.execute_reply.started": "2025-12-14T05:27:03.45183Z",
          "shell.execute_reply": "2025-12-14T05:27:04.175836Z"
        },
        "id": "gv_vVatkdrMG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "from argparse import Namespace\n",
        "from pathlib import Path\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "try:\n",
        "    from metrics_hull import hull_score\n",
        "    from samoyan_pack.stats import xi_correlation\n",
        "    from threeway_tssplit import load_data\n",
        "except:\n",
        "    print('not finding error metric hull')\n",
        "from scipy.stats import spearmanr, pearsonr\n",
        "from sklearn.feature_selection import mutual_info_regression, f_regression\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- BLOCK 1: IMPORTS & SYSTEM SETUP ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "\n",
        "# 1. Device Configuration\n",
        "# We check if a GPU is available. If yes, we use it for speed.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# 2. Reproducibility (Crucial for Finance)\n",
        "# Financial data is noisy. We need to ensure that if we run the code twice,\n",
        "# we get the exact same result, otherwise we can't trust our improvements.\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # These two lines ensure deterministic behavior on CUDA (slightly slower but safer)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "\n",
        "def calculate_optimal_target(df: pl.DataFrame, vol_span: int = 21 * 6) -> pl.Series:\n",
        "    \"\"\"\n",
        "    Calculate optimal position using a Volatility-Adjusted (Kelly-like) approach.\n",
        "\n",
        "    Logic:\n",
        "    1. Calculate Excess Returns.\n",
        "    2. Calculate Rolling/EWM Volatility (Risk).\n",
        "    3. Position = (Excess Return / Risk).\n",
        "       - Higher Return -> Higher Position\n",
        "       - Higher Volatility -> Lower Position (Punishment)\n",
        "    4. Clip result between 0 and 2.\n",
        "    \"\"\"\n",
        "    excess_returns = pl.col(\"forward_returns\") - pl.col(\"risk_free_rate\")\n",
        "    volatility = excess_returns.ewm_std(span=vol_span, adjust=False).fill_null(0) + 1e-6\n",
        "    raw_position = excess_returns / volatility\n",
        "    final_position = raw_position.fill_nan(0)  # .clip(0, 2)\n",
        "\n",
        "    # Return the Series computed from the input DataFrame\n",
        "    return df.select(final_position.alias(\"optimal_position\")).to_series()\n",
        "\n",
        "\n",
        "def create_targets(df: pl.DataFrame, roll_var_window: int = 21):\n",
        "    \"\"\"\n",
        "    Produces cls_label and reg_label and realized_var (rolling var proxy).\n",
        "    Assumes column forward_ret exists (float), risk_free_r and market_forward_excess_returns may exist.\n",
        "    \"\"\"\n",
        "    df = df.with_columns(\n",
        "        [\n",
        "            (pl.col(\"forward_returns\") > 0).cast(pl.Int8).alias(\"cls_label\"),\n",
        "            pl.when(pl.col(\"forward_returns\") > 0)\n",
        "            .then(pl.col(\"forward_returns\"))\n",
        "            .otherwise(0.0)\n",
        "            .alias(\"reg_label\"),\n",
        "        ]\n",
        "    )\n",
        "    # compute realized variance proxy via pandas rolling on forward_returns\n",
        "    pdf = df.to_pandas()\n",
        "    pdf[\"realized_var\"] = (\n",
        "        pdf[\"forward_returns\"]\n",
        "        .rolling(window=roll_var_window, min_periods=1)\n",
        "        .var()\n",
        "        .fillna(method=\"bfill\")\n",
        "        .clip(lower=1e-8)\n",
        "    )\n",
        "    return pl.from_pandas(pdf)\n",
        "\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "print(\"Block 1: Setup Complete.\")\n",
        "trading_days_per_yr = 252\n",
        "trading_days_per_mt = 21\n",
        "# import kaggle_evaluation.default_inference_server\n",
        "\n",
        "try:\n",
        "    if os.environ[\"USER\"].startswith(\"samoyan\"):\n",
        "        args = Namespace(\n",
        "            data_dir=Path(\"./\"),\n",
        "        )\n",
        "except:\n",
        "    args = Namespace(\n",
        "        data_dir=Path(\"/kaggle/input/hull-tactical-market-prediction\"),\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-14T05:29:08.57095Z",
          "iopub.execute_input": "2025-12-14T05:29:08.571371Z",
          "iopub.status.idle": "2025-12-14T05:29:08.591251Z",
          "shell.execute_reply.started": "2025-12-14T05:29:08.571341Z",
          "shell.execute_reply": "2025-12-14T05:29:08.590234Z"
        },
        "id": "N1txH6eXdrMG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas.api.types\n",
        "import talib\n",
        "import polars as pl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "MIN_INVESTMENT = 0\n",
        "MAX_INVESTMENT = 2\n",
        "#  trading_days_per_yr = 252\n",
        "\n",
        "\n",
        "class ParticipantVisibleError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "# ---------- 4. Evaluation: adapted hull_score ----------\n",
        "def hull_score(\n",
        "    solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Adapted from user's function. solution: pandas DF with forward_ret, risk_free_r, market_forward_excess_returns.\n",
        "    submission: pandas DF with columns [row_id_column_name, 'prediction'] (row id must align to solution).\n",
        "    \"\"\"\n",
        "    import pandas as pd, numpy as np\n",
        "\n",
        "    MIN_INVESTMENT = 0\n",
        "    MAX_INVESTMENT = 2\n",
        "    trading_days_per_yr = 252\n",
        "\n",
        "    if row_id_column_name in submission.columns:\n",
        "        submission = submission.set_index(row_id_column_name)\n",
        "    submission = submission.reindex(solution.index)\n",
        "    sol = solution.copy()\n",
        "    sol[\"position\"] = submission[\"prediction\"].values\n",
        "    if sol[\"position\"].max() > MAX_INVESTMENT or sol[\"position\"].min() < MIN_INVESTMENT:\n",
        "        # penalty or error, but here raise\n",
        "        raise ValueError(\"position out of bounds\")\n",
        "    sol[\"strategy_returns\"] = (\n",
        "        sol[\"risk_free_rate\"] * (1 - sol[\"position\"])\n",
        "        + sol[\"position\"] * sol[\"forward_returns\"]\n",
        "    )\n",
        "    strategy_excess_returns = sol[\"strategy_returns\"] - sol[\"risk_free_rate\"]\n",
        "    strategy_excess_cumulative = (1 + strategy_excess_returns).prod()\n",
        "    strategy_mean_excess_return = strategy_excess_cumulative ** (1 / len(sol)) - 1\n",
        "    strategy_std = sol[\"strategy_returns\"].std()\n",
        "    if strategy_std == 0:\n",
        "        return 0.0\n",
        "    sharpe = strategy_mean_excess_return / strategy_std * np.sqrt(trading_days_per_yr)\n",
        "    strategy_volatility = float(strategy_std * np.sqrt(trading_days_per_yr) * 100)\n",
        "    market_excess_returns = sol[\"market_forward_excess_returns\"]\n",
        "    market_excess_cumulative = (1 + market_excess_returns).prod()\n",
        "    market_mean_excess_return = market_excess_cumulative ** (1 / len(sol)) - 1\n",
        "    market_std = sol[\"forward_returns\"].std()\n",
        "    market_volatility = float(market_std * np.sqrt(trading_days_per_yr) * 100)\n",
        "    if market_volatility == 0:\n",
        "        return 0.0\n",
        "    excess_vol = max(0, strategy_volatility / market_volatility - 1.2)\n",
        "    vol_penalty = 1 + excess_vol\n",
        "    return_gap = max(\n",
        "        0,\n",
        "        (market_mean_excess_return - strategy_mean_excess_return)\n",
        "        * 100\n",
        "        * trading_days_per_yr,\n",
        "    )\n",
        "    return_penalty = 1 + (return_gap**2) / 100\n",
        "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
        "    print(\n",
        "        \"return_penalty: \",\n",
        "        return_penalty,\n",
        "        \"\\nvol_penalty: \",\n",
        "        vol_penalty,\n",
        "        \"\\nstrategy_volatility: \",\n",
        "        strategy_volatility,\n",
        "        \"\\nmarket_volatility: \",\n",
        "        market_volatility,\n",
        "    )\n",
        "    print(\"=\" * 30)\n",
        "    return min(float(adjusted_sharpe), 1_000_000)\n",
        "\n",
        "\n",
        "class RollingWindowMinMaxTrainSplit:\n",
        "    \"\"\"\n",
        "    A custom time series cross-validation splitter that generates rolling windows\n",
        "    with explicit training, testing, and standardization sets.\n",
        "\n",
        "    It maintains the training set size between a minimum (`min_train_size`)\n",
        "    and a maximum (`max_train_size`) number of groups. It shifts the entire\n",
        "    window (standardization, train, purge, test) forward by `test_size` groups\n",
        "    for each fold.\n",
        "\n",
        "    It includes:\n",
        "    - A 'standardization' set preceding the training set, of a fixed `standardization_size`.\n",
        "    - A 'purge' period to prevent data leakage between the training and test sets.\n",
        "\n",
        "    This splitter assumes 'groups' represent ordered time steps (e.g., days, weeks)\n",
        "    and that all data points within a group belong to that time step.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        min_train_size=252,\n",
        "        max_train_size=None,\n",
        "        test_size=180,\n",
        "        purge=1,\n",
        "        standardization_size=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the splitter with training, test, purge, and standardization size parameters.\n",
        "\n",
        "        Parameters:\n",
        "        - min_train_size (int): Minimum number of unique groups required in the training set.\n",
        "                                The training window will dynamically adjust to be at least this size,\n",
        "                                within the max_train_size constraint. Must be positive.\n",
        "        - max_train_size (int): Maximum number of unique groups allowed in the training set.\n",
        "                                The training window will not extend further back than this size.\n",
        "                                Must be greater than or equal to `min_train_size`.\n",
        "        - test_size (int): Number of unique groups in the test set for each fold. Must be positive.\n",
        "        - purge (int): Number of unique groups to skip between the end of the training set\n",
        "                       and the start of the test set. This gap helps avoid forward-looking\n",
        "                       data leakage (e.g., target values influencing features). Must be non-negative.\n",
        "                       A value of 0 means no gap (contiguous train-test).\n",
        "        - standardization_size (int): Number of unique groups in the standardization set. This set\n",
        "                                      immediately precedes the training set and is used for\n",
        "                                      calculating normalization parameters. Must be non-negative.\n",
        "                                      A value of 0 means no separate standardization set will be yielded.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If any size parameter is invalid (e.g., non-positive,\n",
        "                        or max_train_size < min_train_size).\n",
        "        \"\"\"\n",
        "        if not isinstance(min_train_size, int) or min_train_size <= 0:\n",
        "            raise ValueError(\"min_train_size must be a positive integer.\")\n",
        "        if not isinstance(max_train_size, int) or max_train_size < min_train_size:\n",
        "            raise ValueError(\n",
        "                \"max_train_size must be an integer greater than or equal to min_train_size.\"\n",
        "            )\n",
        "        if not isinstance(test_size, int) or test_size <= 0:\n",
        "            raise ValueError(\"test_size must be a positive integer.\")\n",
        "        if not isinstance(purge, int) or purge < 0:\n",
        "            raise ValueError(\"purge must be a non-negative integer.\")\n",
        "        if not isinstance(standardization_size, int) or standardization_size < 0:\n",
        "            raise ValueError(\"standardization_size must be a non-negative integer.\")\n",
        "\n",
        "        self.min_train_size = min_train_size\n",
        "        if max_train_size:\n",
        "            self.max_train_size = max_train_size\n",
        "        else:\n",
        "            self.max_train_size = min_train_size\n",
        "        self.test_size = test_size\n",
        "        self.purge = purge\n",
        "        if standardization_size is None:\n",
        "            self.standardization_size = self.max_train_size\n",
        "        else:\n",
        "            self.standardization_size = standardization_size\n",
        "\n",
        "    def split(self, X, y=None, groups=None):\n",
        "        \"\"\"\n",
        "        Generates indices to split data into standardization, training, and test sets.\n",
        "\n",
        "        The method yields three NumPy arrays for each fold.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input data. Not directly used for splitting logic, but required\n",
        "             for scikit-learn compatibility.\n",
        "        - y: Target variable. Optional, not used for splitting.\n",
        "        - groups: An array-like object of time-based group identifiers (e.g., sorted dates\n",
        "                  or sequential integers). Each element corresponds to a row in X,\n",
        "                  and rows with the same group identifier are treated as part of\n",
        "                  the same time step for splitting. Must be provided.\n",
        "\n",
        "        Yields:\n",
        "        - standardization_idx (np.ndarray): NumPy array of integer indices for the\n",
        "                                            standardization set. This array will be empty\n",
        "                                            if `standardization_size` is 0.\n",
        "        - train_idx (np.ndarray): NumPy array of integer indices for the training set.\n",
        "        - test_idx (np.ndarray): NumPy array of integer indices for the test set.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the 'groups' parameter is not provided.\n",
        "        \"\"\"\n",
        "        if groups is None:\n",
        "            raise ValueError(\n",
        "                \"The 'groups' parameter must be provided with time-based identifiers.\"\n",
        "            )\n",
        "\n",
        "        groups_arr = np.asarray(groups)\n",
        "        unique_groups = np.unique(groups_arr)\n",
        "        n_unique_groups = len(unique_groups)\n",
        "\n",
        "        # Pre-map unique group values to their original indices for efficient lookup.\n",
        "        group_to_original_indices = {\n",
        "            g: np.where(groups_arr == g)[0] for g in unique_groups\n",
        "        }\n",
        "\n",
        "        # Calculate the minimum total number of groups required for at least one valid split.\n",
        "        # This includes standardization set, minimum train set, purge, and test set.\n",
        "        min_required_total_groups = (\n",
        "            self.standardization_size\n",
        "            + self.min_train_size\n",
        "            + self.purge\n",
        "            + self.test_size\n",
        "        )\n",
        "        if n_unique_groups < min_required_total_groups:\n",
        "            # Not enough data to form even the first valid split with all components.\n",
        "            return  # Yields nothing\n",
        "\n",
        "        # 'test_start_group_idx' refers to the index within 'unique_groups' where\n",
        "        # the current test set begins.\n",
        "        # The first test set cannot start until all preceding sets (standardization, min_train, purge)\n",
        "        # are accounted for.\n",
        "        test_start_group_idx = (\n",
        "            self.standardization_size + self.min_train_size + self.purge\n",
        "        )\n",
        "\n",
        "        while test_start_group_idx + self.test_size <= n_unique_groups:\n",
        "            # 1. Test Set groups\n",
        "            # These are the group identifiers for the test period.\n",
        "            test_groups_for_fold = unique_groups[\n",
        "                test_start_group_idx : test_start_group_idx + self.test_size\n",
        "            ]\n",
        "\n",
        "            # 2. Training Set groups\n",
        "            # The exclusive end index of the training set groups (within unique_groups).\n",
        "            # It's positioned immediately before the purge period.\n",
        "            train_end_group_idx = test_start_group_idx - self.purge\n",
        "\n",
        "            # The inclusive start index of the training set groups (within unique_groups).\n",
        "            # This must be:\n",
        "            # - At least 0 (cannot go before the first group).\n",
        "            # - At least `self.standardization_size` (to leave space for the standardization set).\n",
        "            # - At most `train_end_group_idx - self.min_train_size` (to ensure minimum train size).\n",
        "            # - At least `train_end_group_idx - self.max_train_size` (to respect maximum train size).\n",
        "\n",
        "            # The earliest a training set can start is either 0 or after the standardization set.\n",
        "            # The latest it can start (to satisfy max_train_size) is (train_end_group_idx - max_train_size).\n",
        "            train_start_group_idx = max(\n",
        "                self.standardization_size,  # Must leave space for standardization set\n",
        "                train_end_group_idx\n",
        "                - self.max_train_size,  # Respect max_train_size lookback\n",
        "            )\n",
        "\n",
        "            # Validate the calculated training window for the current fold.\n",
        "            # If the window size is less than min_train_size, this fold is invalid.\n",
        "            if (train_end_group_idx - train_start_group_idx) < self.min_train_size:\n",
        "                # This can happen if `train_end_group_idx` is too early in `unique_groups`\n",
        "                # or if the constraints for `train_start_group_idx` make the window too small.\n",
        "                break  # Cannot form a valid training set for this test window, so stop.\n",
        "\n",
        "            train_groups_for_fold = unique_groups[\n",
        "                train_start_group_idx:train_end_group_idx\n",
        "            ]\n",
        "\n",
        "            # 3. Standardization Set groups\n",
        "            # The exclusive end of the standardization set is the inclusive start of the training set.\n",
        "            standardization_end_group_idx = train_start_group_idx\n",
        "            # The inclusive start of the standardization set.\n",
        "            standardization_start_group_idx = (\n",
        "                standardization_end_group_idx - self.standardization_size\n",
        "            )\n",
        "\n",
        "            # This check is implicitly covered by the `train_start_group_idx = max(self.standardization_size, ...)`\n",
        "            # but is a good final sanity check or if `standardization_size` is 0.\n",
        "            if standardization_start_group_idx < 0:\n",
        "                # Should not be reached with the current `train_start_group_idx` calculation\n",
        "                # unless `standardization_size` is intended to be 0 and no groups are needed.\n",
        "                # However, if `standardization_size` is > 0 and `train_start_group_idx` is 0, this catches it.\n",
        "                break\n",
        "\n",
        "            standardization_groups_for_fold = unique_groups[\n",
        "                standardization_start_group_idx:standardization_end_group_idx\n",
        "            ]\n",
        "\n",
        "            # Collect the original data indices corresponding to these selected groups.\n",
        "            standardization_idx = np.concatenate(\n",
        "                [group_to_original_indices[g] for g in standardization_groups_for_fold]\n",
        "            )\n",
        "            train_idx = np.concatenate(\n",
        "                [group_to_original_indices[g] for g in train_groups_for_fold]\n",
        "            )\n",
        "            test_idx = np.concatenate(\n",
        "                [group_to_original_indices[g] for g in test_groups_for_fold]\n",
        "            )\n",
        "\n",
        "            yield standardization_idx, train_idx, test_idx\n",
        "\n",
        "            # Move the entire window forward by `test_size` groups for the next fold.\n",
        "            test_start_group_idx += self.test_size\n",
        "\n",
        "    def get_n_splits(self, X=None, y=None, groups=None):\n",
        "        \"\"\"\n",
        "        Returns the total number of splitting iterations (folds) that will be generated.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Ignored.\n",
        "        - y: Ignored.\n",
        "        - groups: Array-like of time-based group identifiers. Must be provided.\n",
        "\n",
        "        Returns:\n",
        "        - n_splits (int): The total number of folds.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the 'groups' parameter is not provided.\n",
        "        \"\"\"\n",
        "        if groups is None:\n",
        "            raise ValueError(\"The 'groups' parameter must be provided.\")\n",
        "\n",
        "        groups_arr = np.asarray(groups)\n",
        "        unique_groups = np.unique(groups_arr)\n",
        "        n_unique_groups = len(unique_groups)\n",
        "\n",
        "        n_splits = 0\n",
        "        # Replicate the exact logic from the 'split' method to ensure consistency.\n",
        "        min_required_total_groups = (\n",
        "            self.standardization_size\n",
        "            + self.min_train_size\n",
        "            + self.purge\n",
        "            + self.test_size\n",
        "        )\n",
        "        if n_unique_groups < min_required_total_groups:\n",
        "            return 0\n",
        "\n",
        "        test_start_group_idx = (\n",
        "            self.standardization_size + self.min_train_size + self.purge\n",
        "        )\n",
        "\n",
        "        while test_start_group_idx + self.test_size <= n_unique_groups:\n",
        "            train_end_group_idx = test_start_group_idx - self.purge\n",
        "\n",
        "            train_start_group_idx = max(\n",
        "                self.standardization_size, train_end_group_idx - self.max_train_size\n",
        "            )\n",
        "\n",
        "            if (train_end_group_idx - train_start_group_idx) < self.min_train_size:\n",
        "                break\n",
        "\n",
        "            standardization_end_group_idx = train_start_group_idx\n",
        "            standardization_start_group_idx = (\n",
        "                standardization_end_group_idx - self.standardization_size\n",
        "            )\n",
        "\n",
        "            if standardization_start_group_idx < 0:\n",
        "                break\n",
        "\n",
        "            n_splits += 1\n",
        "            test_start_group_idx += self.test_size\n",
        "        return n_splits\n",
        "\n",
        "\n",
        "def load_data(df, showplot=False, target_col: str = \"target\"):\n",
        "    # print(df.schema)\n",
        "    # Core rolling features (keeping essential ones, removing redundant)\n",
        "    df_fe = df.with_columns(\n",
        "        [\n",
        "            # Rolling means - keep key timeframes\n",
        "            pl.col(target_col).rolling_mean(4).alias(\"rolling_mean_5\"),\n",
        "            pl.col(target_col).rolling_mean(21).alias(\"rolling_mean_21\"),  # Monthly\n",
        "            pl.col(target_col)\n",
        "            .rolling_mean(63)\n",
        "            .alias(\"rolling_mean_50\"),  # Key technical level\n",
        "            pl.col(target_col)\n",
        "            .rolling_mean(252)\n",
        "            .alias(\"rolling_mean_252\"),  # Long-term trend\n",
        "            # Rolling std - focus on key periods\n",
        "            pl.col(target_col).rolling_std(4).alias(\"rolling_std_5\"),\n",
        "            pl.col(target_col).rolling_std(21).alias(\"rolling_std_21\"),\n",
        "            pl.col(target_col).rolling_std(63).alias(\"rolling_std_50\"),\n",
        "            # Rolling min/max - key support/resistance levels\n",
        "            pl.col(target_col).rolling_min(4).alias(\"rolling_min_5\"),\n",
        "            pl.col(target_col).rolling_min(21).alias(\"rolling_min_21\"),\n",
        "            pl.col(target_col).rolling_min(63).alias(\"rolling_min_50\"),\n",
        "            pl.col(target_col).rolling_max(4).alias(\"rolling_max_5\"),\n",
        "            pl.col(target_col).rolling_max(21).alias(\"rolling_max_21\"),\n",
        "            pl.col(target_col).rolling_max(63).alias(\"rolling_max_50\"),\n",
        "            pl.col(target_col).rolling_min(252).alias(\"rolling_min_252\"),\n",
        "            pl.col(target_col).rolling_max(252).alias(\"rolling_max_252\"),\n",
        "            pl.col(target_col).rolling_skew(4).alias(\"rolling_skew_5\"),\n",
        "            pl.col(target_col).rolling_skew(21).alias(\"rolling_skew_21\"),\n",
        "            pl.col(target_col).rolling_skew(63).alias(\"rolling_skew_50\"),\n",
        "            # pl.col(target_col).kurtosis(4).alias(\"rolling_kurtosis_5\"),\n",
        "            # pl.col(target_col).kurtosis(21).alias(\"rolling_kurtosis_21\"),\n",
        "            # pl.col(target_col).kurtosis(63).alias(\"rolling_kurtosis_50\"),\n",
        "            # Price lags - keep recent history\n",
        "            pl.col(target_col).shift(1).alias(\"value_lag_1\"),\n",
        "            pl.col(target_col).shift(2).alias(\"value_lag_2\"),\n",
        "            pl.col(target_col).shift(4).alias(\"value_lag_5\"),\n",
        "            # Date features\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Price-based features\n",
        "    df_fe = df_fe.with_columns(\n",
        "        [\n",
        "            # Returns\n",
        "            (pl.col(target_col) / pl.col(\"value_lag_1\") - 1).alias(\"return_1d\"),\n",
        "            (pl.col(target_col) / pl.col(\"value_lag_2\") - 1).alias(\"return_2d\"),\n",
        "            (pl.col(target_col) / pl.col(\"value_lag_5\") - 1).alias(\"return_5d\"),\n",
        "            # Log returns (better for modeling)\n",
        "            (pl.col(target_col) / pl.col(\"value_lag_1\")).log().alias(\"log_return_1d\"),\n",
        "            (pl.col(target_col) / pl.col(\"value_lag_5\")).log().alias(\"log_return_5d\"),\n",
        "            # Volatility measures\n",
        "            pl.col(target_col).rolling_std(21).alias(\"volatility_21d\"),\n",
        "            (\n",
        "                pl.col(target_col).rolling_std(4) / pl.col(target_col).rolling_std(21)\n",
        "            ).alias(\"vol_ratio_5_21\"),\n",
        "            # Price position within recent range\n",
        "            (\n",
        "                (pl.col(target_col) - pl.col(\"rolling_min_21\"))\n",
        "                / (pl.col(\"rolling_max_21\") - pl.col(\"rolling_min_21\"))\n",
        "            ).alias(\"price_position_21d\"),\n",
        "            # Distance from moving averages (normalized)\n",
        "            (\n",
        "                (pl.col(target_col) - pl.col(\"rolling_mean_21\"))\n",
        "                / pl.col(\"rolling_std_21\")\n",
        "            ).alias(\"zscore_21d\"),\n",
        "            (\n",
        "                (pl.col(target_col) - pl.col(\"rolling_mean_50\"))\n",
        "                / pl.col(\"rolling_std_50\")\n",
        "            ).alias(\"zscore_50d\"),\n",
        "            # Momentum indicators\n",
        "            (pl.col(target_col) - pl.col(\"rolling_mean_5\")).alias(\"momentum_5d\"),\n",
        "            (pl.col(target_col) - pl.col(\"rolling_mean_21\")).alias(\"momentum_21d\"),\n",
        "            (\n",
        "                (pl.col(target_col) - pl.col(\"value_lag_5\")) / pl.col(\"value_lag_5\")\n",
        "            ).alias(\"roc_5d\"),\n",
        "            (pl.col(\"rolling_mean_5\") > pl.col(\"rolling_mean_21\")).alias(\n",
        "                \"short_above_medium\"\n",
        "            ),\n",
        "            (pl.col(\"rolling_mean_21\") > pl.col(\"rolling_mean_50\")).alias(\n",
        "                \"medium_above_long\"\n",
        "            ),\n",
        "            (pl.col(\"rolling_mean_50\") > pl.col(\"rolling_mean_252\")).alias(\n",
        "                \"trend_bullish\"\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    # Advanced technical features\n",
        "    df_fe = df_fe.with_columns(\n",
        "        [\n",
        "            # Relative strength vs different timeframes\n",
        "            (pl.col(target_col) / pl.col(\"rolling_mean_5\")).alias(\n",
        "                \"relative_strength_5d\"\n",
        "            ),\n",
        "            (pl.col(target_col) / pl.col(\"rolling_mean_21\")).alias(\n",
        "                \"relative_strength_21d\"\n",
        "            ),\n",
        "            (pl.col(target_col) / pl.col(\"rolling_mean_50\")).alias(\n",
        "                \"relative_strength_50d\"\n",
        "            ),\n",
        "            # Bollinger Band position\n",
        "            (\n",
        "                (pl.col(target_col) - pl.col(\"rolling_mean_21\"))\n",
        "                / (2 * pl.col(\"rolling_std_21\"))\n",
        "            ).alias(\"bb_position\"),\n",
        "            # Support/Resistance levels\n",
        "            ((pl.col(target_col) - pl.col(\"rolling_max_5\")).abs() <= 3).alias(\n",
        "                \"at_5d_high\"\n",
        "            ),\n",
        "            ((pl.col(target_col) - pl.col(\"rolling_min_5\")).abs() <= 3).alias(\n",
        "                \"at_5d_low\"\n",
        "            ),\n",
        "            ((pl.col(target_col) - pl.col(\"rolling_max_21\")).abs() <= 3).alias(\n",
        "                \"at_21d_high\"\n",
        "            ),\n",
        "            ((pl.col(target_col) - pl.col(\"rolling_min_21\")).abs() <= 3).alias(\n",
        "                \"at_21d_low\"\n",
        "            ),\n",
        "            # Trend strength\n",
        "            (pl.col(\"rolling_mean_5\").diff() > 0).alias(\"ma5_trending_up\"),\n",
        "            (pl.col(\"rolling_mean_21\").diff() > 0).alias(\"ma21_trending_up\"),\n",
        "            # Volume-like proxy (using price volatility)\n",
        "            (pl.col(\"rolling_std_5\") / pl.col(\"rolling_std_21\")).alias(\n",
        "                \"volatility_ratio\"\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    value_scaled = df_fe[target_col].to_numpy()\n",
        "\n",
        "    # Hilbert Transform\n",
        "    hts1, hts2 = talib.HT_SINE(value_scaled)\n",
        "\n",
        "    # Bollinger Bands\n",
        "    bb_upper, bb_middle, bb_lower = talib.BBANDS(value_scaled, timeperiod=20)\n",
        "\n",
        "    # MACD\n",
        "    macd_line, macd_signal, macd_hist = talib.MACD(value_scaled)\n",
        "\n",
        "    # Additional technical indicators\n",
        "    rsi = talib.RSI(value_scaled, timeperiod=14)\n",
        "    stoch_k, stoch_d = talib.STOCH(value_scaled, value_scaled, value_scaled)\n",
        "    williams_r = talib.WILLR(value_scaled, value_scaled, value_scaled)\n",
        "    cci = talib.CCI(value_scaled, value_scaled, value_scaled)\n",
        "\n",
        "    # Add all technical indicators\n",
        "    df_fe = df_fe.with_columns(\n",
        "        [\n",
        "            pl.Series(name=\"ht_sine\", values=hts1),\n",
        "            pl.Series(name=\"ht_leadsine\", values=hts2),\n",
        "            pl.Series(name=\"bb_upper\", values=bb_upper),\n",
        "            pl.Series(name=\"bb_lower\", values=bb_lower),\n",
        "            pl.Series(name=\"macd_line\", values=macd_line),\n",
        "            pl.Series(name=\"macd_signal\", values=macd_signal),\n",
        "            pl.Series(name=\"macd_histogram\", values=macd_hist),\n",
        "            pl.Series(name=\"rsi\", values=rsi),\n",
        "            pl.Series(name=\"stoch_k\", values=stoch_k),\n",
        "            pl.Series(name=\"stoch_d\", values=stoch_d),\n",
        "            pl.Series(name=\"williams_r\", values=williams_r),\n",
        "            pl.Series(name=\"cci\", values=cci),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Derived features from technical indicators\n",
        "    df_fe = df_fe.with_columns(\n",
        "        [\n",
        "            # Bollinger Band signals\n",
        "            (pl.col(target_col) > pl.col(\"bb_upper\")).alias(\"bb_breakout_upper\"),\n",
        "            (pl.col(target_col) < pl.col(\"bb_lower\")).alias(\"bb_breakout_lower\"),\n",
        "            (\n",
        "                (pl.col(target_col) - pl.col(\"bb_lower\"))\n",
        "                / (pl.col(\"bb_upper\") - pl.col(\"bb_lower\"))\n",
        "            ).alias(\"bb_percent\"),\n",
        "            # MACD signals\n",
        "            (pl.col(\"macd_line\") > pl.col(\"macd_signal\")).alias(\"macd_bullish\"),\n",
        "            (pl.col(\"macd_histogram\") > 0).alias(\"macd_hist_positive\"),\n",
        "            # RSI levels\n",
        "            (pl.col(\"rsi\") > 70).alias(\"rsi_overbought\"),\n",
        "            (pl.col(\"rsi\") < 30).alias(\"rsi_oversold\"),\n",
        "            # Stochastic signals\n",
        "            (pl.col(\"stoch_k\") > pl.col(\"stoch_d\")).alias(\"stoch_bullish\"),\n",
        "            ((pl.col(\"stoch_k\") > 80) & (pl.col(\"stoch_d\") > 80)).alias(\n",
        "                \"stoch_overbought\"\n",
        "            ),\n",
        "            ((pl.col(\"stoch_k\") < 20) & (pl.col(\"stoch_d\") < 20)).alias(\n",
        "                \"stoch_oversold\"\n",
        "            ),\n",
        "            # Multi-timeframe confirmation\n",
        "            ((pl.col(\"ma5_trending_up\")) & (pl.col(\"ma21_trending_up\"))).alias(\n",
        "                \"multi_ma_bullish\"\n",
        "            ),\n",
        "            (\n",
        "                (pl.col(\"rsi\") > 50) & (pl.col(\"macd_line\") > pl.col(\"macd_signal\"))\n",
        "            ).alias(\"momentum_confluence\"),\n",
        "            (\n",
        "                pl.col(\"log_return_1d\").rolling_std(4)\n",
        "                / pl.col(\"log_return_1d\").rolling_std(21)\n",
        "            ).alias(\"volatility_spread\"),\n",
        "            pl.col(\"momentum_5d\").diff().alias(\"momentum_acceleration_5d\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if showplot:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(df_fe[\"stateDate\"], df_fe[\"value\"], label=\"Original Data\")\n",
        "        plt.plot(df_fe[\"stateDate\"], df_fe[\"talib_EMA\"], label=\"hts1\")\n",
        "        plt.plot(df_fe[\"stateDate\"], df_fe[\"talib_SMA\"], label=\"hts2\")\n",
        "        plt.title(\n",
        "            f\"next month contract Scaled {target_col} history close Value with EMA and SMA\"\n",
        "        )\n",
        "        plt.xlabel(\"State Date\")\n",
        "\n",
        "    return df_fe\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-14T05:29:09.226081Z",
          "iopub.execute_input": "2025-12-14T05:29:09.226401Z",
          "iopub.status.idle": "2025-12-14T05:29:09.276651Z",
          "shell.execute_reply.started": "2025-12-14T05:29:09.226376Z",
          "shell.execute_reply": "2025-12-14T05:29:09.275504Z"
        },
        "jupyter": {
          "source_hidden": true
        },
        "id": "LSouWfzqdrMG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train = pl.read_csv(args.data_dir / \"train.csv\")\n",
        "test = pl.read_csv(args.data_dir / \"test.csv\")\n",
        "\n",
        "train = train.with_columns(pl.selectors.string().cast(pl.Float64))\n",
        "train = train.with_columns(\n",
        "    (pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"),\n",
        "    (pl.col(\"M11\") / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3)).alias(\"U2\"),\n",
        "    pl.col(\"forward_returns\").shift(1).alias(\"lagged_forward_returns\"),\n",
        "    pl.col(\"risk_free_rate\").shift(1).alias(\"lagged_risk_free_rate\"),\n",
        "    pl.col(\"market_forward_excess_returns\")\n",
        "    .shift(1)\n",
        "    .alias(\"lagged_market_forward_excess_returns\"),\n",
        "    # ((pl.col(\"forward_returns\") - pl.col(\"risk_free_rate\")) > 0).alias(\"if_pos_r\"),\n",
        "    # pl.col(\"forward_returns\").log().alias(\"log_forward_returns\"),\n",
        ")\n",
        "train = train.with_columns(\n",
        "    (pl.col(\"date_id\") // trading_days_per_yr).alias(\"yrno\"),\n",
        "    np.sin(np.pi * 2 * pl.col(\"date_id\") % trading_days_per_yr).alias(\"day_of_yr_sin\"),\n",
        "    np.cos(np.pi * 2 * pl.col(\"date_id\") % trading_days_per_yr).alias(\"day_of_yr_cos\"),\n",
        "    np.sin(np.pi / trading_days_per_yr * pl.col(\"date_id\") % trading_days_per_yr).alias(\n",
        "        \"day_of_yr_sin2\"\n",
        "    ),\n",
        "    np.cos(np.pi / trading_days_per_yr * pl.col(\"date_id\") % trading_days_per_yr).alias(\n",
        "        \"day_of_yr_cos2\"\n",
        "    ),\n",
        ").with_columns(\n",
        "    (pl.col(\"day_of_yr_cos2\") / 100 + (1 - (1 / (pl.int_range(0, pl.count()) + 1e-6))) * 100).alias(\n",
        "        \"magic_up\"\n",
        "    )\n",
        ")\n",
        "train = create_targets(train)\n",
        "train = train.with_columns(target=calculate_optimal_target(train))\n",
        "train = load_data(train, target_col=\"lagged_market_forward_excess_returns\")\n",
        "\n",
        "# ------------------------------------- x ------------------------------------ #\n",
        "\n",
        "exclude_cols = [\n",
        "    \"date_id\",\n",
        "    \"forward_returns\",\n",
        "    \"risk_free_rate\",\n",
        "    \"market_forward_excess_returns\",\n",
        "    \"if_pos_r\",\n",
        "    \"target\",\n",
        "    \"cls_label\",\n",
        "    \"reg_label\",\n",
        "    \"realized_var\",\n",
        "]\n",
        "feature_cols = [c for c in train.columns if c not in exclude_cols]\n",
        "\n",
        "#\n",
        "# *---------------------------------------------------------------------------- #\n",
        "# *                               for null check                                #\n",
        "# *---------------------------------------------------------------------------- #\n",
        "train = train.fill_nan(None)\n",
        "\n",
        "print(f\"\\nTotal features available: {len(feature_cols)}\")\n",
        "\n",
        "display(train.select(feature_cols).null_count())\n",
        "# px.histogram(train.select(feature_cols).null_count().transpose(), nbins=60)\n",
        "display(\n",
        "    train.select([col for col in train.columns if train[col].null_count() < 1205])[\n",
        "        1200:, :\n",
        "    ]\n",
        "    .null_count()\n",
        "    .transpose()\n",
        "    .max()\n",
        ")\n",
        "# samo, skip first 1200 rows to remove some null val cols.\n",
        "train = train.select([col for col in train.columns if train[col].null_count() < 1205])[\n",
        "    1200:, :\n",
        "]\n",
        "train = train.fill_null(strategy=\"forward\")\n",
        "\n",
        "\n",
        "# 3. Hyperparameter Configuration\n",
        "# We define all our settings here. This makes tuning the model much easier later.\n",
        "class CFG:\n",
        "    # Data Settings\n",
        "    input_dim = len(feature_cols)  # Example: 94 stock characteristics (from Gu et al.)\n",
        "    output_dim = 1  # Regression: Predicting 1 value (e.g., Return)\n",
        "    # Model Architecture (Shallow & Robust)\n",
        "    hidden_layers = [64, 32, 16]  # The \"Funnel\" shape\n",
        "    dropout_prob = 0.4  # High dropout to prevent overfitting\n",
        "    # Training Settings\n",
        "    learning_rate = 0.001\n",
        "    batch_size = 128\n",
        "    epochs = 20\n",
        "    # Loss Settings\n",
        "    huber_delta = 1.0  # Threshold for Huber Loss\n",
        "    # samo: trees\n",
        "    verbose = 33\n",
        "    num_boost_round = 3000\n",
        "    early_stopping_rounds = 50\n",
        "    exclude_cols = exclude_cols"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-14T05:29:09.278275Z",
          "iopub.execute_input": "2025-12-14T05:29:09.278547Z",
          "iopub.status.idle": "2025-12-14T05:29:09.470062Z",
          "shell.execute_reply.started": "2025-12-14T05:29:09.278517Z",
          "shell.execute_reply": "2025-12-14T05:29:09.469016Z"
        },
        "id": "NE9sxRoYdrMH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "feature_cols = [c for c in train.columns if c not in CFG.exclude_cols]\n",
        "target_col = \"target\"\n",
        "\n",
        "# Extract features and target as NumPy arrays\n",
        "X = train.select(feature_cols).to_numpy()\n",
        "y = train.select(target_col).to_numpy().flatten()\n",
        "print(f\"\\nX shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "try:\n",
        "    feature_sets = joblib.load(\"/kaggle/input/hull-feature-select-dict/feature_select_dict.py\")\n",
        "except:\n",
        "    raise SystemError\n",
        "    feature_sets = feature_selection_create_df()\n",
        "    joblib.dump(feature_sets, \"feature_select_dict.py\")\n",
        "\n",
        "\n",
        "CFG.cat_cols = (\n",
        "    train[feature_cols].select(pl.selectors.by_dtype([pl.Int64, pl.Boolean])).columns\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-14T05:29:48.902352Z",
          "iopub.execute_input": "2025-12-14T05:29:48.90316Z",
          "iopub.status.idle": "2025-12-14T05:29:48.922091Z",
          "shell.execute_reply.started": "2025-12-14T05:29:48.903132Z",
          "shell.execute_reply": "2025-12-14T05:29:48.921029Z"
        },
        "id": "OHPPN0ijdrMH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cat\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "# ----------------------------------- limix ---------------------------------- #\n",
        "# import os\n",
        "\n",
        "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "# sys.path.append(\"/mnt/c/d2/Limix\")\n",
        "# from inference.predictor import LimiXPredictor\n",
        "\n",
        "# MAX_CONTEXT = 1024  # Reduced slightly to be safe for CUDA kernels\n",
        "# EVAL_BATCH_SIZE = 16  # <--- NEW: Predict in chunks of 128 to avoid CUDA crash\n",
        "# model_file = \"/mnt/c/d2/Limix/model_weight/LimiX-2M.ckpt\"\n",
        "# inference_config = \"/mnt/c/d2/Limix/config/reg_default_noretrieval.json\"\n",
        "# ----------------------------------- limix ---------------------------------- #\n",
        "\n",
        "\n",
        "def build_models(random_state: int = 42):\n",
        "    return {\n",
        "        \"ElasticNet\": ElasticNet(\n",
        "            alpha=0.01,\n",
        "            l1_ratio=0.1,\n",
        "            max_iter=1_000_000,\n",
        "            random_state=random_state,\n",
        "            selection=\"random\",\n",
        "        ),\n",
        "        \"XGBoost\": dict(\n",
        "            n_estimators=10_000,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=2,\n",
        "            objective=\"reg:pseudohubererror\",\n",
        "            n_jobs=-1,\n",
        "            random_state=random_state,\n",
        "            early_stopping_rounds=50,\n",
        "        ),\n",
        "        \"XGBoost1\": dict(\n",
        "            n_estimators=1_000,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=6,\n",
        "            objective=\"reg:pseudohubererror\",\n",
        "            n_jobs=-1,\n",
        "            random_state=random_state,\n",
        "            early_stopping_rounds=50,\n",
        "            colsample_bytree=0.8,  # Use only 70% of features per tree\n",
        "            subsample=0.8,  # Use only 70% of rows per tree\n",
        "            reg_alpha=0.1,  # L1 Regularization\n",
        "            reg_lambda=1.0,  # L2 Regularization\n",
        "            gamma=0.1,  # Min loss reduction required to make a split\n",
        "            # min_child_weight=1,\n",
        "        ),\n",
        "        \"LightGBM\": dict(\n",
        "            n_estimators=10_000,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=2,\n",
        "            objective=\"huber\",\n",
        "            n_jobs=-1,\n",
        "            random_state=random_state,\n",
        "            verbose=-1,\n",
        "        ),\n",
        "        \"LightGBM1\": dict(\n",
        "            n_estimators=1_000,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=6,\n",
        "            objective=\"huber\",\n",
        "            n_jobs=-1,\n",
        "            random_state=random_state,\n",
        "            verbose=-1,\n",
        "            colsample_bytree=0.8,\n",
        "            subsample=0.8,\n",
        "            reg_alpha=0.5,\n",
        "            reg_lambda=1,\n",
        "            # min_split_gain=0.1,  # Higher threshold for deeper trees\n",
        "            # min_child_samples=40,  #\n",
        "        ),\n",
        "        \"CatBoost\": dict(\n",
        "            iterations=10_000,\n",
        "            learning_rate=0.05,\n",
        "            depth=2,\n",
        "            loss_function=\"Huber:delta=1.0\",\n",
        "            random_seed=random_state,\n",
        "            early_stopping_rounds=50,\n",
        "            # rsm=.7,\n",
        "            # subsample=.7,\n",
        "        ),\n",
        "        \"CatBoost1\": dict(\n",
        "            iterations=10_000,\n",
        "            learning_rate=0.05,\n",
        "            depth=6,\n",
        "            loss_function=\"Huber:delta=1.0\",\n",
        "            random_seed=random_state,\n",
        "            early_stopping_rounds=50,\n",
        "            rsm=0.8,\n",
        "            l2_leaf_reg=0.8,\n",
        "            subsample=0.8,\n",
        "        ),\n",
        "        # \"nn\": FinancialRegressor(input_dim=X_train_scaled.shape[1]),\n",
        "        # \"limix\": LimiXPredictor(\n",
        "        #     device=device,\n",
        "        #     model_path=model_file,\n",
        "        #     inference_config=inference_config,\n",
        "        #     categorical_features_indices=None,\n",
        "        # ),\n",
        "    }\n",
        "\n",
        "\n",
        "# trainingfunctions\n",
        "def lightgbm_training(\n",
        "    x_train: pd.DataFrame,\n",
        "    y_train: pd.DataFrame,\n",
        "    x_valid: pd.DataFrame,\n",
        "    y_valid: pd.DataFrame,\n",
        "    cat_cols: list = [],\n",
        "):\n",
        "    # display(pd.Series(train.dtypes).value_counts())\n",
        "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_cols)\n",
        "    lgb_valid = lgb.Dataset(x_valid, y_valid, categorical_feature=cat_cols)\n",
        "\n",
        "    model = lgb.train(\n",
        "        params=CFG.model_params,\n",
        "        train_set=lgb_train,\n",
        "        num_boost_round=CFG.num_boost_round,\n",
        "        valid_sets=[lgb_train, lgb_valid],\n",
        "        valid_names=[\"train\", \"val\"],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(\n",
        "                stopping_rounds=CFG.early_stopping_rounds, verbose=CFG.verbose\n",
        "            ),\n",
        "            lgb.log_evaluation(CFG.verbose),\n",
        "        ],\n",
        "    )\n",
        "    # Predict validation\n",
        "    valid_pred = model.predict(x_valid)\n",
        "    return model, valid_pred\n",
        "\n",
        "\n",
        "def xgboost_training(\n",
        "    x_train: pd.DataFrame,\n",
        "    y_train: pd.DataFrame,\n",
        "    x_valid: pd.DataFrame,\n",
        "    y_valid: pd.DataFrame,\n",
        "):\n",
        "    xgb_train = xgb.DMatrix(data=x_train, label=y_train)\n",
        "    xgb_valid = xgb.DMatrix(data=x_valid, label=y_valid)\n",
        "    model = xgb.train(\n",
        "        CFG.model_params,\n",
        "        dtrain=xgb_train,\n",
        "        num_boost_round=CFG.num_boost_round,\n",
        "        evals=[(xgb_train, \"train\"), (xgb_valid, \"eval\")],\n",
        "        early_stopping_rounds=CFG.early_stopping_rounds,\n",
        "        verbose_eval=CFG.verbose,\n",
        "    )\n",
        "    # Predict validation\n",
        "    valid_pred = model.predict(xgb.DMatrix(x_valid))\n",
        "    return model, valid_pred\n",
        "\n",
        "\n",
        "def catboost_training(\n",
        "    x_train: pd.DataFrame,\n",
        "    y_train: pd.DataFrame,\n",
        "    x_valid: pd.DataFrame,\n",
        "    y_valid: pd.DataFrame,\n",
        "    cat_cols: list = [],\n",
        "):\n",
        "    cat_train = cat.Pool(data=x_train, label=y_train, cat_features=cat_cols)\n",
        "    cat_valid = cat.Pool(data=x_valid, label=y_valid, cat_features=cat_cols)\n",
        "    model = cat.CatBoostRegressor(**CFG.model_params)\n",
        "    model.fit(\n",
        "        cat_train,\n",
        "        eval_set=[cat_valid],\n",
        "        early_stopping_rounds=CFG.early_stopping_rounds,\n",
        "        verbose=CFG.verbose,\n",
        "        use_best_model=True,\n",
        "    )\n",
        "    # Predict validation\n",
        "    valid_pred = model.predict(x_valid)\n",
        "    return model, valid_pred\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# HELPER: BATCHED PREDICTION\n",
        "# ==========================================\n",
        "def limix_batched_predict(\n",
        "    model,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    X_valid,\n",
        "    batch_size=512,\n",
        "):\n",
        "    preds = []\n",
        "\n",
        "    for i in range(0, len(X_valid), batch_size):\n",
        "        xb = X_valid[i : i + batch_size]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            p = model.predict(\n",
        "                X_train,\n",
        "                y_train,\n",
        "                xb,\n",
        "                task_type=\"Regression\",\n",
        "            )\n",
        "\n",
        "        preds.append(p)\n",
        "\n",
        "    return np.concatenate(preds, axis=0)\n",
        "\n",
        "\n",
        "def train_models_reg(\n",
        "    std_df: pl.DataFrame,\n",
        "    train: pl.DataFrame,\n",
        "    valid: pl.DataFrame,\n",
        "    FEATURES: list,\n",
        "    _cat_cols: list,\n",
        "    target_col: str,\n",
        "):\n",
        "    _num_cols = [str(c) for c in np.setdiff1d(FEATURES, _cat_cols)]\n",
        "    _cat_cols = [str(c) for c in _cat_cols]\n",
        "    FEATURES = [str(c) for c in FEATURES]\n",
        "\n",
        "    s_train = std_df[_num_cols].to_numpy()\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(s_train)\n",
        "\n",
        "    X_train = train[_num_cols].to_numpy()\n",
        "    X_valid = valid[_num_cols].to_numpy()\n",
        "    y_train = train[target_col].to_numpy()\n",
        "    y_valid = valid[target_col].to_numpy()\n",
        "\n",
        "    def scale_num(df):\n",
        "        return pd.DataFrame(\n",
        "            scaler.transform(df[_num_cols].to_numpy()),\n",
        "            columns=_num_cols,\n",
        "            index=df.to_pandas().index if isinstance(df, pl.DataFrame) else df.index,\n",
        "        )\n",
        "\n",
        "    X_train_num = scale_num(train)\n",
        "    X_valid_num = scale_num(valid)\n",
        "\n",
        "    y_train = train[target_col].to_numpy()\n",
        "    y_valid = valid[target_col].to_numpy()\n",
        "\n",
        "    # ===============================\n",
        "    # 3. Encode categorical features (NN-safe)\n",
        "    # ===============================\n",
        "    if _cat_cols:\n",
        "        cat_encoder = OrdinalEncoder(\n",
        "            handle_unknown=\"use_encoded_value\",\n",
        "            unknown_value=-1,\n",
        "            encoded_missing_value=-1,\n",
        "            dtype=np.int64,\n",
        "        )\n",
        "        X_train_cat = cat_encoder.fit_transform(\n",
        "            train.select(_cat_cols).to_pandas().astype(str)\n",
        "        )\n",
        "        X_valid_cat = cat_encoder.transform(\n",
        "            valid.select(_cat_cols).to_pandas().astype(str)\n",
        "        )\n",
        "\n",
        "        X_train_cat = pd.DataFrame(\n",
        "            X_train_cat,\n",
        "            columns=_cat_cols,\n",
        "            index=train.to_pandas().index,\n",
        "        )\n",
        "        X_valid_cat = pd.DataFrame(\n",
        "            X_valid_cat,\n",
        "            columns=_cat_cols,\n",
        "            index=valid.to_pandas().index,\n",
        "        )\n",
        "\n",
        "        X_train = pd.concat([X_train_num, X_train_cat], axis=1)\n",
        "        X_valid = pd.concat([X_valid_num, X_valid_cat], axis=1)\n",
        "\n",
        "        num_dim = len(_num_cols)\n",
        "        cat_dim = len(_cat_cols)\n",
        "        categorical_idx = list(range(num_dim, num_dim + cat_dim))\n",
        "    else:\n",
        "        X_train = X_train_num\n",
        "        X_valid = X_valid_num\n",
        "        categorical_idx = None\n",
        "        cat_encoder = None\n",
        "    # ===============================\n",
        "    # 4. Safety checks (critical for CUDA)\n",
        "    # ===============================\n",
        "    if cat_encoder is not None:\n",
        "        for i, c in enumerate(_cat_cols):\n",
        "            max_allowed = len(cat_encoder.categories_[i])\n",
        "            assert X_train[c].max() <= max_allowed\n",
        "            assert X_valid[c].max() <= max_allowed\n",
        "\n",
        "    # = torch params\n",
        "    returns_train = train[\"forward_returns\"].to_numpy()\n",
        "    input_dim = X_train.shape[1]\n",
        "    WINDOW_SIZE = 21  # for torch models.\n",
        "    # Define the dictionary of models\n",
        "    trained_models = {}\n",
        "    valid_scores = {}  # 1. Initialize collector\n",
        "    model_dict = build_models()\n",
        "    print(f\"{'='*10} Starting Training Loop {'='*10}\\n\")\n",
        "\n",
        "    for name, model in model_dict.items():\n",
        "        print(f\"Training {name} model...\")\n",
        "        CFG.model_params = model\n",
        "        if \"XGB\" in name:\n",
        "            model, position = xgboost_training(X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "        elif \"LightGBM\" in name:\n",
        "            model, position = lightgbm_training(\n",
        "                X_train, y_train, X_valid, y_valid, _cat_cols\n",
        "            )\n",
        "\n",
        "        elif \"CatBoost\" in name:\n",
        "            model, position = catboost_training(\n",
        "                X_train, y_train, X_valid, y_valid, _cat_cols\n",
        "            )\n",
        "\n",
        "        elif \"limix\" in name or \"nn\" in name:\n",
        "            # NN models consume:\n",
        "            # - float32 numericals\n",
        "            # - int64 categoricals\n",
        "            import os\n",
        "\n",
        "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "            model = LimiXPredictor(\n",
        "                device=torch.device(\"cpu\"),\n",
        "                model_path=model_file,\n",
        "                inference_config=inference_config,\n",
        "                categorical_features_indices=categorical_idx,\n",
        "            )\n",
        "            position = limix_batched_predict(\n",
        "                model,\n",
        "                X_train.values,\n",
        "                y_train.astype(\"float32\"),\n",
        "                X_valid.values,\n",
        "                batch_size=512,  # start small\n",
        "            )\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "            position = model.predict(X_valid)\n",
        "\n",
        "        # # 2. Check Training Score\n",
        "        # train_score = model.score(X_train_scaled, y_train)\n",
        "        # print(f\"{name} Train R score: {train_score:.4f}\")\n",
        "\n",
        "        # 3. Predict on Validation Data (passed in function)\n",
        "        position = np.clip(position, 0.0, 2.0)\n",
        "        # 4. Prepare Validation Submission DataFrame for hull_score\n",
        "        # We use the 'valid' dataframe passed to the function, not global 'test'\n",
        "        valid_pd = valid.to_pandas() if isinstance(valid, pl.DataFrame) else valid\n",
        "        # Create a dataframe matching the index of the validation set\n",
        "        valid_sub = pd.DataFrame(\n",
        "            position, index=valid_pd.index, columns=[\"prediction\"]\n",
        "        ).reset_index()\n",
        "        # 5. Calculate Hull Score on Validation Set\n",
        "        # Assuming hull_score is defined elsewhere in your code\n",
        "        score_ = hull_score(valid_pd, valid_sub, \"index\")\n",
        "        valid_scores[name] = score_\n",
        "        print(f\"{name} Validation Hull Score: {score_:.4f}\")\n",
        "        print(\"=\" * 30)\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        trained_models[name] = model\n",
        "        print()\n",
        "        # = ------------------------------- test sub part ------------------------------ #\n",
        "        # if \"XGB\" in name:\n",
        "        #     position = np.clip(\n",
        "        #         model.predict(xgb.DMatrix(test_df[X_train_scaled.columns])), 0.0, 2.0\n",
        "        #     )\n",
        "        # else:\n",
        "        #     position = np.clip(model.predict(test_df[X_train_scaled.columns]), 0.0, 2.0)\n",
        "        # # # Create a dataframe matching the index of the validation set\n",
        "        # test_sub = pd.DataFrame(\n",
        "        #     position,\n",
        "        #     index=train.tail(180).select(\"date_id\")[\"date_id\"].to_list(),\n",
        "        #     columns=[\"prediction\"],\n",
        "        # ).reset_index()\n",
        "        # print(\n",
        "        #     \"# ------------------------------------- = ------------------------------------ #\"\n",
        "        # )\n",
        "        # print(\"test score\")\n",
        "        # print(hull_score(train.tail(180).to_pandas(), test_sub, \"date_id\"))\n",
        "        # print(\n",
        "        #     \"# ------------------------------------- = ------------------------------------ #\"\n",
        "        # )\n",
        "    return trained_models, scaler, cat_encoder, valid_scores"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-14T05:29:57.816631Z",
          "iopub.execute_input": "2025-12-14T05:29:57.816988Z",
          "iopub.status.idle": "2025-12-14T05:29:58.436048Z",
          "shell.execute_reply.started": "2025-12-14T05:29:57.816964Z",
          "shell.execute_reply": "2025-12-14T05:29:58.434806Z"
        },
        "id": "jiaoKCLXdrMH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trading_days_per_yr = 252\n",
        "# from threeway_tssplit import RollingWindowMinMaxTrainSplit\n",
        "\n",
        "\n",
        "feature_sets_to_test = list(feature_sets.keys())\n",
        "\n",
        "# Store all results\n",
        "all_results = []\n",
        "num_fold_trained = 0\n",
        "train_output = []\n",
        "# https://arxiv.org/html/2502.17493v2  A Novel Loss Function for Deep Learning Based Daily Stock Trading System\n",
        "ts = RollingWindowMinMaxTrainSplit(\n",
        "    min_train_size=trading_days_per_yr * 6,\n",
        "    max_train_size=trading_days_per_yr * 6,\n",
        "    test_size=21 * 6,\n",
        "    purge=0,\n",
        "    standardization_size=trading_days_per_yr * 1,\n",
        ")\n",
        "\n",
        "\n",
        "TRAIN = False\n",
        "if TRAIN:\n",
        "    for i in feature_sets_to_test:\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\" TRAINING MODELS\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"\\nConfiguration:\")\n",
        "        print(f\"  Feature sets: {i}\")\n",
        "        print()\n",
        "        FEATURES = feature_sets[i]\n",
        "        _cat_cols = list(np.intersect1d(FEATURES, CFG.cat_cols))\n",
        "\n",
        "        for ind, (std_i, tra_i, val_i) in enumerate(\n",
        "            ts.split(train, groups=range(len(train)))\n",
        "        ):\n",
        "            if (ind % 5 != 0):\n",
        "                continue\n",
        "\n",
        "            print(\"=\" * 20, f\"fold {ind}\", \"=\" * 20)\n",
        "            print(\n",
        "                \"=\" * 15,\n",
        "                f\"tra idx: {tra_i.min(), tra_i.max()}; val idx: {val_i.max()}; std: {std_i.max()}\",\n",
        "                \"=\" * 20,\n",
        "            )\n",
        "            m, _scaler, _cat_encoder, _scores = train_models_reg(\n",
        "                std_df=train[std_i],\n",
        "                train=train[tra_i],\n",
        "                valid=train[val_i],\n",
        "                FEATURES=FEATURES,\n",
        "                _cat_cols=_cat_cols,\n",
        "                target_col=target_col,\n",
        "            )\n",
        "            _scores[\"fold\"] = ind\n",
        "            _scores[\"feature_nums\"] = i\n",
        "            all_results.append(_scores)\n",
        "\n",
        "            print(\"\\n\\n\\n\")\n",
        "            train_output.append(\n",
        "                {\n",
        "                    \"feature_list\": i,\n",
        "                    \"fold\": ind,\n",
        "                    \"tra_idx\": f\"{tra_i.min(), tra_i.max()}\",\n",
        "                    \"models\": m,\n",
        "                    \"scaler\": _scaler,\n",
        "                    \"encoder\": _cat_encoder,\n",
        "                    \"score\": _scores,\n",
        "                }\n",
        "            )\n",
        "            # display(train[std_i, \"date_id\"])\n",
        "            # display(train[tra_i, \"date_id\"])\n",
        "            # display(train[val_i, \"date_id\"])\n",
        "            num_fold_trained += 1\n",
        "    print(\"\\nAll models trained successfully!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-14T05:30:13.774793Z",
          "iopub.execute_input": "2025-12-14T05:30:13.775505Z",
          "iopub.status.idle": "2025-12-14T05:30:13.788131Z",
          "shell.execute_reply.started": "2025-12-14T05:30:13.775475Z",
          "shell.execute_reply": "2025-12-14T05:30:13.786328Z"
        },
        "id": "Ymp1EkmYdrMI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_output = joblib.load( \"/kaggle/input/hull-feature-select-dict/tr_reg_enc_1214.py\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-14T05:30:26.318092Z",
          "iopub.execute_input": "2025-12-14T05:30:26.319329Z",
          "iopub.status.idle": "2025-12-14T05:30:26.603486Z",
          "shell.execute_reply.started": "2025-12-14T05:30:26.319287Z",
          "shell.execute_reply": "2025-12-14T05:30:26.602639Z"
        },
        "id": "GENWarzSdrMI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test = (\n",
        "    pl.read_csv(args.data_dir / \"train.csv\")\n",
        "    .with_columns(pl.selectors.string().cast(pl.Float64))\n",
        "    .with_columns(\n",
        "        pl.lit(False).alias(\"is_scored\"),\n",
        "        pl.col(\"forward_returns\").shift(1).alias(\"lagged_forward_returns\"),\n",
        "        pl.col(\"risk_free_rate\").shift(1).alias(\"lagged_risk_free_rate\"),\n",
        "        pl.col(\"market_forward_excess_returns\")\n",
        "        .shift(1)\n",
        "        .alias(\"lagged_market_forward_excess_returns\"),\n",
        "    )\n",
        "    .drop([\"forward_returns\", \"risk_free_rate\", \"market_forward_excess_returns\"])\n",
        "    .vstack(\n",
        "        pl.read_csv(args.data_dir / \"test.csv\").with_columns(\n",
        "            pl.selectors.string().cast(pl.Float64)\n",
        "        )\n",
        "    )\n",
        ")\n",
        "test = (\n",
        "    test.with_columns(\n",
        "        (pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"),\n",
        "        (pl.col(\"M11\") / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3)).alias(\n",
        "            \"U2\"\n",
        "        ),\n",
        "    )\n",
        "    .with_columns(\n",
        "        (pl.col(\"date_id\") // trading_days_per_yr).alias(\"yrno\"),\n",
        "        np.sin(np.pi * 2 * pl.col(\"date_id\") % trading_days_per_yr).alias(\n",
        "            \"day_of_yr_sin\"\n",
        "        ),\n",
        "        np.cos(np.pi * 2 * pl.col(\"date_id\") % trading_days_per_yr).alias(\n",
        "            \"day_of_yr_cos\"\n",
        "        ),\n",
        "        np.sin(\n",
        "            np.pi / trading_days_per_yr * pl.col(\"date_id\") % trading_days_per_yr\n",
        "        ).alias(\"day_of_yr_sin2\"),\n",
        "        np.cos(\n",
        "            np.pi / trading_days_per_yr * pl.col(\"date_id\") % trading_days_per_yr\n",
        "        ).alias(\"day_of_yr_cos2\"),\n",
        "    )\n",
        "    .with_columns(\n",
        "        (\n",
        "            pl.col(\"day_of_yr_cos2\") / 100\n",
        "            + (1 - (1 / (pl.int_range(0, pl.count()) + 1e-6))) * 100\n",
        "        ).alias(\"magic_up\")\n",
        "    )\n",
        ").fill_nan(None)\n",
        "test = load_data(test, target_col=\"lagged_market_forward_excess_returns\")\n",
        "test = test.fill_null(strategy=\"forward\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-14T05:30:38.692605Z",
          "iopub.execute_input": "2025-12-14T05:30:38.692985Z",
          "iopub.status.idle": "2025-12-14T05:30:38.874099Z",
          "shell.execute_reply.started": "2025-12-14T05:30:38.692961Z",
          "shell.execute_reply": "2025-12-14T05:30:38.872009Z"
        },
        "id": "4IEOQR-sdrMI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_test(\n",
        "    test_df: pl.DataFrame,\n",
        "    FEATURES: list[str],\n",
        "    cat_cols: list[str],\n",
        "    scaler,\n",
        "    encoder=None,\n",
        "):\n",
        "    FEATURES = [str(c) for c in FEATURES]\n",
        "    cat_cols = [str(c) for c in cat_cols]\n",
        "    num_cols = [c for c in FEATURES if c not in cat_cols]\n",
        "    # -------------------------------\n",
        "    # 1. Numeric\n",
        "    # -------------------------------\n",
        "    X_num = pd.DataFrame(\n",
        "        scaler.transform(test_df[num_cols].to_numpy()),\n",
        "        columns=num_cols,\n",
        "        index=test_df.to_pandas().index,\n",
        "    )\n",
        "    # -------------------------------\n",
        "    # 2. Categorical (optional)\n",
        "    # -------------------------------\n",
        "    if cat_cols:\n",
        "        if encoder is None:\n",
        "            raise ValueError(\"Categorical columns exist but encoder is None\")\n",
        "        X_cat = encoder.transform(test_df.select(cat_cols).to_pandas().astype(str))\n",
        "        # shift for NN-safe / embedding-safe\n",
        "        X_cat = X_cat + 1\n",
        "        X_cat = pd.DataFrame(\n",
        "            X_cat,\n",
        "            columns=cat_cols,\n",
        "            index=X_num.index,\n",
        "        )\n",
        "        X = pd.concat([X_num, X_cat], axis=1)\n",
        "    else:\n",
        "        X = X_num\n",
        "    return X\n",
        "\n",
        "\n",
        "def predict_model(model, model_name, X):\n",
        "    if \"XGB\" in model_name:\n",
        "        import xgboost as xgb\n",
        "\n",
        "        dmat = xgb.DMatrix(X)\n",
        "        return model.predict(dmat)\n",
        "\n",
        "    elif \"LightGBM\" in model_name:\n",
        "        return model.predict(X)\n",
        "\n",
        "    elif \"CatBoost\" in model_name:\n",
        "        return model.predict(X)\n",
        "\n",
        "    else:  # sklearn (ElasticNet, etc.)\n",
        "        return model.predict(X)\n",
        "\n",
        "\n",
        "def compute_weights(score_dict, eps=1e-6):\n",
        "    \"\"\"\n",
        "    score_dict: {model_name: float_score, ...}\n",
        "    returns: {model_name: normalized_weight}\n",
        "    \"\"\"\n",
        "    for k, v in score_dict.items():\n",
        "        if isinstance(v, float):\n",
        "            raw = {k: 1.0 / (v + eps)}\n",
        "\n",
        "    s = sum(raw.values())\n",
        "    return {k: v / s for k, v in raw.items()}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-14T05:35:04.505597Z",
          "iopub.execute_input": "2025-12-14T05:35:04.506142Z",
          "iopub.status.idle": "2025-12-14T05:35:04.782912Z",
          "shell.execute_reply.started": "2025-12-14T05:35:04.506112Z",
          "shell.execute_reply": "2025-12-14T05:35:04.781764Z"
        },
        "id": "34qllKvodrMI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict(test: pl.DataFrame) -> float:\n",
        "    \"\"\"Replace this function with your inference code.\n",
        "    You can return either a Pandas or Polars dataframe, though Polars is recommended for performance.\n",
        "    Each batch of predictions (except the very first) must be returned within 5 minutes of the batch features being provided.\n",
        "    \"\"\"\n",
        "    test_raw = test.clone()\n",
        "    try:\n",
        "        test = (\n",
        "            pl.read_csv(args.data_dir / \"train.csv\")\n",
        "            .with_columns(pl.selectors.string().cast(pl.Float64))\n",
        "            .with_columns(\n",
        "                pl.lit(False).alias(\"is_scored\"),\n",
        "                pl.col(\"forward_returns\").shift(1).alias(\"lagged_forward_returns\"),\n",
        "                pl.col(\"risk_free_rate\").shift(1).alias(\"lagged_risk_free_rate\"),\n",
        "                pl.col(\"market_forward_excess_returns\")\n",
        "                .shift(1)\n",
        "                .alias(\"lagged_market_forward_excess_returns\"),\n",
        "            )\n",
        "            .drop([\"forward_returns\", \"risk_free_rate\", \"market_forward_excess_returns\"])\n",
        "            .vstack(\n",
        "                   test.with_columns(\n",
        "                    pl.selectors.string().cast(pl.Float64)\n",
        "                )\n",
        "            )\n",
        "        ).sort('is_scored', descending=True).unique(\"date_id\")\n",
        "        test = (\n",
        "            test.with_columns(\n",
        "                (pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"),\n",
        "                (pl.col(\"M11\") / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3)).alias(\n",
        "                    \"U2\"\n",
        "                ),\n",
        "            )\n",
        "            .with_columns(\n",
        "                (pl.col(\"date_id\") // trading_days_per_yr).alias(\"yrno\"),\n",
        "                np.sin(np.pi * 2 * pl.col(\"date_id\") % trading_days_per_yr).alias(\n",
        "                    \"day_of_yr_sin\"\n",
        "                ),\n",
        "                np.cos(np.pi * 2 * pl.col(\"date_id\") % trading_days_per_yr).alias(\n",
        "                    \"day_of_yr_cos\"\n",
        "                ),\n",
        "                np.sin(\n",
        "                    np.pi / trading_days_per_yr * pl.col(\"date_id\") % trading_days_per_yr\n",
        "                ).alias(\"day_of_yr_sin2\"),\n",
        "                np.cos(\n",
        "                    np.pi / trading_days_per_yr * pl.col(\"date_id\") % trading_days_per_yr\n",
        "                ).alias(\"day_of_yr_cos2\"),\n",
        "            )\n",
        "            .with_columns(\n",
        "                (\n",
        "                    pl.col(\"day_of_yr_cos2\") / 100\n",
        "                    + (1 - (1 / (pl.int_range(0, pl.count()) + 1e-6))) * 100\n",
        "                ).alias(\"magic_up\")\n",
        "            )\n",
        "        ).fill_nan(None)\n",
        "        test = load_data(test, target_col=\"lagged_market_forward_excess_returns\")\n",
        "        test = test.fill_null(strategy=\"forward\").sort('date_id')\n",
        "        all_test_preds = []\n",
        "\n",
        "        for fold_obj in train_output:\n",
        "            FEATURES = fold_obj[\"feature_list\"]\n",
        "            models = fold_obj[\"models\"]\n",
        "            scaler = fold_obj[\"scaler\"]\n",
        "            encoder = fold_obj.get(\"encoder\", None)\n",
        "            score_dict = fold_obj[\"score\"]\n",
        "            cat_cols = list(np.intersect1d(FEATURES, CFG.cat_cols))\n",
        "            test_sub = test.filter(pl.col(\"is_scored\") == True).select(FEATURES)\n",
        "            X_test = preprocess_test(\n",
        "                test_df=test_sub,\n",
        "                FEATURES=FEATURES,\n",
        "                cat_cols=cat_cols,\n",
        "                scaler=scaler,\n",
        "                encoder=encoder,\n",
        "            )\n",
        "            weights = compute_weights(score_dict)\n",
        "            fold_pred = np.zeros(len(X_test), dtype=np.float32)\n",
        "\n",
        "            for name, model in models.items():\n",
        "                if name not in weights:\n",
        "                    continue\n",
        "\n",
        "                p = predict_model(model, name, X_test)\n",
        "                p = np.clip(p, 0.0, 2.0)\n",
        "\n",
        "                fold_pred += weights[name] * p\n",
        "\n",
        "            all_test_preds.append(fold_pred)\n",
        "        final_test_pred = np.mean(np.vstack(all_test_preds), axis=0)\n",
        "        test_out = test.filter(pl.col(\"is_scored\") == True).select(\"date_id\").to_pandas()\n",
        "        test_out[\"prediction\"] = final_test_pred\n",
        "        # display(test_out)\n",
        "        print(final_test_pred[0])\n",
        "        return float(final_test_pred[0])\n",
        "    except Exception as e:\n",
        "        print(\"=\" * 10)\n",
        "        print(e)\n",
        "        display(test_raw)\n",
        "\n",
        "        return 0.0\n",
        "    # return float(final_test_pred[0])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-14T05:48:31.212696Z",
          "iopub.execute_input": "2025-12-14T05:48:31.213347Z",
          "iopub.status.idle": "2025-12-14T05:48:31.231533Z",
          "shell.execute_reply.started": "2025-12-14T05:48:31.213315Z",
          "shell.execute_reply": "2025-12-14T05:48:31.230121Z"
        },
        "id": "rfWY6gtOdrMI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import kaggle_evaluation.default_inference_server\n",
        "\n",
        "\n",
        "# When your notebook is run on the hidden test set, inference_server.serve must be called within 15 minutes of the notebook starting\n",
        "# or the gateway will throw an error. If you need more than 15 minutes to load your model you can do so during the very\n",
        "# first `predict` call, which does not have the usual 1 minute response deadline.\n",
        "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
        "\n",
        "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
        "    inference_server.serve()\n",
        "else:\n",
        "    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-14T05:48:31.23787Z",
          "iopub.execute_input": "2025-12-14T05:48:31.238302Z",
          "iopub.status.idle": "2025-12-14T05:48:34.930778Z",
          "shell.execute_reply.started": "2025-12-14T05:48:31.238275Z",
          "shell.execute_reply": "2025-12-14T05:48:34.929751Z"
        },
        "id": "resm0Yz5drMI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# def predict(test: pl.DataFrame) -> float:\n",
        "#     \"\"\"Replace this function with your inference code.\n",
        "#     You can return either a Pandas or Polars dataframe, though Polars is recommended for performance.\n",
        "#     Each batch of predictions (except the very first) must be returned within 5 minutes of the batch features being provided.\n",
        "#     \"\"\"\n",
        "#     return 0.0\n",
        "\n",
        "\n",
        "# # When your notebook is run on the hidden test set, inference_server.serve must be called within 15 minutes of the notebook starting\n",
        "# # or the gateway will throw an error. If you need more than 15 minutes to load your model you can do so during the very\n",
        "# # first `predict` call, which does not have the usual 1 minute response deadline.\n",
        "# inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
        "\n",
        "# if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
        "#     inference_server.serve()\n",
        "# else:\n",
        "#     inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-14T05:38:33.30901Z",
          "iopub.execute_input": "2025-12-14T05:38:33.309334Z",
          "iopub.status.idle": "2025-12-14T05:38:33.419525Z",
          "shell.execute_reply.started": "2025-12-14T05:38:33.309311Z",
          "shell.execute_reply": "2025-12-14T05:38:33.417065Z"
        },
        "id": "3Z0pFrPOdrMI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "eWVGFHYodrMI"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}